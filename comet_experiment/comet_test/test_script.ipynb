{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3feb8bbe",
   "metadata": {},
   "source": [
    "# Test Script for Trained COMET Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c9c29",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0521ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "from comet import load_from_checkpoint\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4473e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract WMT data\n",
    "wmt_data = {}\n",
    "language_pairs = ['de-en', 'cs-en', 'fi-en', 'ru-en']\n",
    "srcs = []\n",
    "hyps = []\n",
    "refs = []\n",
    "gold = []\n",
    "langs = []\n",
    "for language_pair in language_pairs:\n",
    "    with open(f'data/DAseg.newstest2015.source.{language_pair}', encoding='utf-8') as f:\n",
    "              srcs += [line.strip() for line in f]\n",
    "    with open(f'data/DAseg.newstest2015.mt-system.{language_pair}', encoding='utf-8') as f:\n",
    "              hyps += [line.strip() for line in f]\n",
    "    with open(f'data/DAseg.newstest2015.reference.{language_pair}', encoding='utf-8') as f:\n",
    "              refs += [line.strip() for line in f]\n",
    "    with open(f'data/DAseg.newstest2015.human.{language_pair}', encoding='utf-8') as f:\n",
    "              gold += [float(line.strip()) for line in f]\n",
    "    langs += [language_pair]*(len(gold)-len(langs))\n",
    "\n",
    "wmt_data = {'srcs':srcs, 'hyps':hyps, 'refs':refs, 'gold':gold, 'langs':langs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a607bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test dataset\n",
    "test_data = [{\"src\":'', \"mt\":hyp, \"ref\":ref} for src, hyp, ref in zip(wmt_data['srcs'], wmt_data['hyps'], wmt_data['refs'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfdd438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoder model frozen.\n",
      "Encoder model frozen.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Encoder model frozen.\n"
     ]
    }
   ],
   "source": [
    "#Load checkpoint paths\n",
    "minilm_checkpoint_path = '../comet_train/lightning_logs/minilm/checkpoints/last.ckpt'\n",
    "minilm_adapter_checkpoint_path = '../comet_train/lightning_logs/minilm_adapter/checkpoints/last.ckpt'\n",
    "xlmr_checkpoint_path = '../comet_train/lightning_logs/xlmr/checkpoints/last.ckpt'\n",
    "xlmr_adapter_checkpoint_path = '../comet_train/lightning_logs/xlmr_adapter/checkpoints/last.ckpt'\n",
    "#Load checkpoints\n",
    "minilm = load_from_checkpoint(minilm_checkpoint_path)\n",
    "minilm_adapter = load_from_checkpoint(minilm_adapter_checkpoint_path)\n",
    "xlmr = load_from_checkpoint(xlmr_checkpoint_path)\n",
    "xlmr_adapter = load_from_checkpoint(xlmr_adapter_checkpoint_path)\n",
    "#Create a dict containing checkpoints\n",
    "checkpoint_dict = {'minilm':minilm, 'minilm_adapter':minilm_adapter, 'xlmr':xlmr, 'xlmr_adapter':xlmr_adapter}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb7d64",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e9b3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████| 250/250 [03:11<00:00,  1.31it/s]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████| 250/250 [03:20<00:00,  1.24it/s]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████| 250/250 [10:32<00:00,  2.53s/it]\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████| 250/250 [11:57<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "#Create lists to store the evaluation values\n",
    "checkpoint_name_list = []\n",
    "duration_list = []\n",
    "corr_list = []\n",
    "\n",
    "#Evaluation of each of the four models under consideration\n",
    "for checkpoint_name in checkpoint_dict.keys():\n",
    "    #Determine start time\n",
    "    start_time = time.time()\n",
    "    #Compute predictions\n",
    "    seg_scores, sys_score = checkpoint_dict[checkpoint_name].predict(test_data, gpus=0)\n",
    "    #Compute test runtime/duration\n",
    "    duration = time.time() - start_time\n",
    "    #Compute Pearson correlation\n",
    "    corr = pearsonr(seg_scores, wmt_data['gold']).statistic\n",
    "    #Append evaluation values to lists\n",
    "    checkpoint_name_list.append(checkpoint_name)\n",
    "    duration_list.append(duration)\n",
    "    corr_list.append(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "944dfe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the training runtime \n",
    "#----------------------------------------------------------------#\n",
    "#This had to be done manually, because the training via the bash #\n",
    "#did not offer the possibility to save the corresponding runtimes#\n",
    "#----------------------------------------------------------------#\n",
    "#Computation: Over five epochs calculate the sum of minutes times# \n",
    "#             60 (seconds) plus sum of seconds over five epochs  #\n",
    "#----------------------------------------------------------------#\n",
    "dur_train_xlmr = (13 + 34 + 35 + 34 + 34)*60 +  (22 + 46 + 13 + 49 + 31)\n",
    "dur_train_xlmr_adapter = (13 + 35 + 35 + 36 + 37)*60 + (34 + 30 + 38 + 15 + 32) \n",
    "dur_train_minilm = (6 + 14 + 11 + 10 + 10)*60 + (24 + 30 + 47 + 59 + 57)\n",
    "dur_train_minilm_adapter = (5 + 11 + 11 + 11 + 11)*60 + (1 + 21 + 21 + 18 + 15)\n",
    "dur_train_list = [dur_train_minilm, dur_train_minilm_adapter, dur_train_xlmr, dur_train_xlmr_adapter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d50039dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrainDur</th>\n",
       "      <th>TestDur</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>minilm</th>\n",
       "      <td>3277</td>\n",
       "      <td>191.033967</td>\n",
       "      <td>0.692330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minilm_adapter</th>\n",
       "      <td>3016</td>\n",
       "      <td>200.881294</td>\n",
       "      <td>0.692330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr</th>\n",
       "      <td>9161</td>\n",
       "      <td>632.647120</td>\n",
       "      <td>0.649850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlmr_adapter</th>\n",
       "      <td>9509</td>\n",
       "      <td>717.769104</td>\n",
       "      <td>0.687421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TrainDur     TestDur  Correlation\n",
       "minilm              3277  191.033967     0.692330\n",
       "minilm_adapter      3016  200.881294     0.692330\n",
       "xlmr                9161  632.647120     0.649850\n",
       "xlmr_adapter        9509  717.769104     0.687421"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Collect the evalaution values in a dataframe\n",
    "df = pd.DataFrame(list(zip(dur_train_list, duration_list, corr_list)),\n",
    "                  columns =['TrainDur','TestDur', 'Correlation'],\n",
    "                  index = checkpoint_name_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f4eec",
   "metadata": {},
   "source": [
    "## Models contained in checkpoint files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e409e5d",
   "metadata": {},
   "source": [
    "### minilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38b9ef1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"microsoft/Multilingual-MiniLM-L12-H384\",\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 384,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"XLMRobertaTokenizer\",\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250037\n",
       "}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minilm.encoder.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1039cde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"microsoft/Multilingual-MiniLM-L12-H384\",\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 384,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"XLMRobertaTokenizer\",\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250037\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minilm_adapter.encoder.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17fea3",
   "metadata": {},
   "source": [
    "### xlmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9baa39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaConfig {\n",
       "  \"_name_or_path\": \"xlm-roberta-base\",\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"architectures\": [\n",
       "    \"XLMRobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"xlm-roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250002\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr.encoder.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32ddc067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaConfig {\n",
       "  \"_name_or_path\": \"xlm-roberta-base\",\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {\n",
       "      \"en\": \"16eaa0b5fae9ed68\"\n",
       "    },\n",
       "    \"config_map\": {\n",
       "      \"16eaa0b5fae9ed68\": {\n",
       "        \"adapter_residual_before_ln\": false,\n",
       "        \"cross_adapter\": false,\n",
       "        \"inv_adapter\": \"nice\",\n",
       "        \"inv_adapter_reduction_factor\": 2,\n",
       "        \"leave_out\": [],\n",
       "        \"ln_after\": false,\n",
       "        \"ln_before\": false,\n",
       "        \"mh_adapter\": false,\n",
       "        \"non_linearity\": \"relu\",\n",
       "        \"original_ln_after\": true,\n",
       "        \"original_ln_before\": true,\n",
       "        \"output_adapter\": true,\n",
       "        \"reduction_factor\": 2,\n",
       "        \"residual_before_ln\": true\n",
       "      }\n",
       "    },\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"architectures\": [\n",
       "    \"XLMRobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"xlm-roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.12.5\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250002\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_adapter.encoder.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3329a04",
   "metadata": {},
   "source": [
    "In the COMET model configuration for MiniLM trained with adapters, no adapter is displayed. In contrast, this is done for XLM-R trained with adapters. Consequently, in the case of MiniLM, the adapter was not included in the COMET metric. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
